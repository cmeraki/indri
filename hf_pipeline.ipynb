{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b13f3de1-76bf-4050-b718-1f4f30f4201a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model\n",
    "# load audio data\n",
    "# pass through audio tokenizer, prepare tokens according to the task\n",
    "# pass through model\n",
    "# post process outputs based on the task\n",
    "# return text/audio tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "714285f8-fe5e-4c38-89f0-b0cbfbb4ba66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset, Audio\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "from transformers import Pipeline, pipeline\n",
    "from transformers.pipelines import PIPELINE_REGISTRY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5306e94-4561-4776-918d-17abed55b9ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset(\"PolyAI/minds14\", name=\"en-US\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b3011e-2b20-4833-b128-13d3303a9a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "from transformers import MimiModel, GenerationConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd5c2fbe-39c3-4507-84be-bd52fbcdeb2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IndriPipeline(Pipeline):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "        self.audio_tokenizer = MimiModel.from_pretrained('kyutai/mimi').to(device=self.device)\n",
    "\n",
    "        # Ideally all of this should come from model config\n",
    "        self.convert_token = self.tokenizer.encode('[convert]')\n",
    "        self.stop_token = self.tokenizer.encode('[stop]')\n",
    "        self.text_modality_token = self.tokenizer.encode('[text]')\n",
    "        self.acoustic_modality_token = self.tokenizer.encode('[mimi]')\n",
    "        self.num_codebooks = 8\n",
    "        self.audio_offset = 50257\n",
    "\n",
    "        self.model.generation_config = GenerationConfig(\n",
    "            eos_token_id=self.stop_token,\n",
    "            max_length=kwargs.get('max_length', 1024),\n",
    "            temperature=kwargs.get('temperature', 0.5),\n",
    "            top_k=kwargs.get('top_k', 15),\n",
    "            do_sample=kwargs.get('do_sample', True)\n",
    "        )\n",
    "\n",
    "    def _sanitize_parameters(self, **kwargs):\n",
    "        task = kwargs.get('task', 'tts')\n",
    "        assert task in ['tts', 'asr'], f'Task must be one of tts, asr. You provided: {task}'\n",
    "\n",
    "        speaker = kwargs.get('speaker', '[spkr_unk]')\n",
    "\n",
    "        preprocess_kwargs = {\n",
    "            'task': task,\n",
    "            'speaker': speaker\n",
    "        }\n",
    "\n",
    "        return preprocess_kwargs, {}, {}\n",
    "\n",
    "    def _prepare_tts_tokens(self, text_tokens, speaker):\n",
    "        input_tokens = np.hstack([\n",
    "            self.text_modality_token,\n",
    "            text_tokens,\n",
    "            self.convert_token,\n",
    "            self.acoustic_modality_token,\n",
    "            self.tokenizer.encode(speaker)\n",
    "        ])\n",
    "\n",
    "        return input_tokens.tolist()\n",
    "\n",
    "    def _prepare_asr_tokens(self, audio_tokens):\n",
    "        pass\n",
    "\n",
    "    def _sanitize_text(self, text):\n",
    "        text = text.lower()\n",
    "        text = re.sub(r'\\n+', ' ', text)\n",
    "        text = re.sub(r'[ \\t]+', ' ', text)\n",
    "\n",
    "        text = re.sub(r'([,\\.?])+', r'\\1', text)\n",
    "\n",
    "        return text.strip()\n",
    "\n",
    "    def _deserialize_tokens(self, tokens):\n",
    "        tokens = tokens.view(self.num_codebooks, -1)\n",
    "        min_shape = torch.min(tokens.shape[1], dim=1).values\n",
    "        acoustic_tokens = tokens[:, :min_shape] - 2048 * torch.arange(num_codebooks, device=tokens.device).unsqueeze(1)\n",
    "\n",
    "        return acoustic_tokens\n",
    "\n",
    "    def preprocess(self, inputs, speaker, task):\n",
    "        if task == 'tts':\n",
    "            input_text = self._sanitize_text(inputs)\n",
    "            input_tokens = self.tokenizer.encode(inputs)\n",
    "            task_tokens = self._prepare_tts_tokens(input_tokens, speaker)\n",
    "            task_tokens = torch.tensor(task_tokens).unsqueeze(0)\n",
    "\n",
    "        elif task == 'asr':\n",
    "            raise ValueError('ASR task is not yet supported')\n",
    "\n",
    "        return {'task_tokens': task_tokens}\n",
    "\n",
    "    def _forward(self, model_inputs, **forward_args):\n",
    "\n",
    "        outputs = self.model.generate(model_inputs['task_tokens'])\n",
    "        outputs = outputs[:, model_inputs['task_tokens'].shape[-1]:]\n",
    "\n",
    "        end = torch.where(outputs == self.stop_token[0])[-1]\n",
    "\n",
    "        if end.shape[-1] > 0:\n",
    "            end = end[0]\n",
    "        else:\n",
    "            end = outputs.shape[-1]\n",
    "\n",
    "        outputs = outputs[:, :end]\n",
    "        outputs -= self.audio_offset\n",
    "\n",
    "        # audio_tokens = self._deserialize_tokens(outputs)\n",
    "        # print(audio_tokens.shape, audio_tokens)\n",
    "        # audio = self.audio_tokenizer.decode(audio_tokens).audio_values\n",
    "\n",
    "        return {\n",
    "            'model_output': outputs,\n",
    "            # 'audio': audio\n",
    "        }\n",
    "\n",
    "    def postprocess(self, model_outputs):\n",
    "        return model_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f8a609-ec92-42bc-b3d3-4c3d1911f346",
   "metadata": {},
   "outputs": [],
   "source": [
    "PIPELINE_REGISTRY.register_pipeline(\n",
    "    \"indri\",\n",
    "    pipeline_class=IndriPipeline,\n",
    "    pt_model=AutoModelForCausalLM\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d72497-a07e-4b38-9df5-702fdba9c3a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = pipeline(\"indri\", model=\"cmeraki/mimi_124m_8cb\", device=torch.device('cuda:0'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09082106-02f6-4fb5-8397-11c2456da86b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "o = pipe([\"my name is romit\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6dff680-c594-4f48-822e-8dedcd78f806",
   "metadata": {},
   "outputs": [],
   "source": [
    "toks = o[0]['model_output'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba307cf-2707-49b2-a14a-af4596a6bb4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "toks.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39954b7c-9bb5-4382-a045-a1295c2da4e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "toks.view(8, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d44995e8-87fc-4071-a8ad-21edd5bf4096",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _deserialize_tokens(tokens):\n",
    "    tokens = tokens.view(8, -1)\n",
    "    min_shape = torch.min(tokens.shape[1], dim=1).values\n",
    "    acoustic_tokens = tokens[:, :min_shape] - 2048 * torch.arange(num_codebooks, device=tokens.device).unsqueeze(1)\n",
    "\n",
    "    return acoustic_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "460c01d0-e6e8-406e-b3c1-cc5d4281aed0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
